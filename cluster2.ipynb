{
 "cells": [
  {
   "cell_type": "raw",
   "id": "81c0218e-a205-46ca-ae0b-25530c473849",
   "metadata": {},
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3554fab1-0190-4ad3-a563-e8caab7c7a61",
   "metadata": {},
   "source": [
    "Ans:Hierarchical clustering-\n",
    "A Hierarchical clustering method works via grouping data into a tree of clusters. Hierarchical clustering begins by treating every data point as a separate cluster. Then, it repeatedly executes the subsequent steps:\n",
    "1.Identify the 2 clusters which can be closest together, and\n",
    "2.Merge the 2 maximum comparable clusters. We need to continue these steps until all the clusters are merged together.\n",
    "\n",
    "Hierarchical clustering algorithm used for small dataset.In Hierarchical Clustering, results are reproducible in Hierarchical clustering.Hierarchical clustering don’t work  as well as, k means when the  shape of the clusters is hyper  spherical.Hierarchical methods can be either divisive or agglomerative."
   ]
  },
  {
   "cell_type": "raw",
   "id": "defbb7fd-db07-4e98-a9fa-d19237da658a",
   "metadata": {},
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd15a1b7-8103-462e-975a-b1222c157089",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1. Agglomerative: Initially consider every data point as an individual Cluster and at every step, merge the nearest pairs of the cluster. (It is a bottom-up method). At first, every dataset is considered an individual entity or cluster. At every iteration, the clusters merge with different clusters until one cluster is formed.\n",
    "\n",
    "The algorithm for Agglomerative Hierarchical Clustering is:\n",
    "1.Calculate the similarity of one cluster with all the other clusters (calculate proximity matrix)\n",
    "2.Consider every data point as an individual cluster\n",
    "3.Merge the clusters which are highly similar or close to each other.\n",
    "4.Recalculate the proximity matrix for each cluster\n",
    "5.Repeat Steps 3 and 4 until only a single cluster remains.\n",
    "\n",
    "2. Divisive: \n",
    "We can say that Divisive Hierarchical clustering is precisely the opposite of Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we take into account all of the data points as a single cluster and in every iteration, we separate the data points from the clusters which aren’t comparable. In the end, we are left with N clusters."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1b1df8b-7080-42aa-9e77-94de8775a22a",
   "metadata": {},
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "794629d2-7dbe-4d55-aa85-82843b005457",
   "metadata": {},
   "source": [
    "Ans:For most common hierarchical clustering , the default distance measure is the Euclidean distance. This is the square root of the sum of the square differences.\n",
    "\n",
    "methods:\n",
    "1.Euclidean Distance-The Euclidean distance between two points calculates the length of a segment connecting the two points. It is the most evident way of representing the distance between two points\n",
    "\n",
    "2.Manhattan Distance-The distance between two points in a grid-based on a strictly horizontal and vertical path. The Manhattan distance is the simple sum of the horizontal and vertical components.\n",
    "\n",
    "3.Minkowski Distance-The Minkowski distance between two variables X and Y is defined as-\n",
    "When p = 1, Minkowski Distance is equivalent to the Manhattan distance, and the case where p = 2, is equivalent to the Euclidean distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "355d01ee-9531-4cdb-8fdd-93421922f62b",
   "metadata": {},
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a051e75-10c6-4273-a5e9-e9611a52cd71",
   "metadata": {},
   "source": [
    "Ans:Dendogram-\n",
    "In the dendrogram locate the largest vertical difference between nodes, and in the middle pass an horizontal line. The number of vertical lines intersecting it is the optimal number of clusters \n",
    "\n",
    "Gap Statistic\n",
    "Calinski-Harabasz Index\n",
    "Davies-Bouldin Index\n",
    "Bayesian information criterion (BIC)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07a2e062-5cbf-489c-acd5-9b534c9f569b",
   "metadata": {},
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1c21150-b00b-40fb-85c8-520b1b4d7c06",
   "metadata": {},
   "source": [
    "Ans:A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clust.ers.\n",
    "Dendogram gives the optimal number of clusters in hierarchical clustering thus we are able to create clusters.Differnt clusters define differnt properties"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc54ba3c-1304-4162-bcd4-1ae783a89235",
   "metadata": {},
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "248f119c-0e5c-4303-9e77-cb853f57d236",
   "metadata": {},
   "source": [
    "Ans: Yes, hierachical clustering be used for both numerical and categorical data\n",
    "\n",
    "Manhattan distance is usually preferred over the more common Euclidean distance when there is high dimensionality in the data. Hamming distance is used to measure the distance between categorical variables, and the Cosine distance metric is mainly used to find the amount of similarity between two data points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "95485062-1034-475e-957e-0f35418bbe5a",
   "metadata": {},
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e3b8dcd-912f-4dd7-9c87-be2ca91c7cee",
   "metadata": {},
   "source": [
    "Ans:Creates hierarchical decomposition of the specified data record of the data object. They can be based on distance or density and continuity. They are divided into cohesion method and division method. If so, this is an outlier. \n",
    "\n",
    "For outlier detection in Hierarchical Clustering, it depends\n",
    "on dendrogram; here we have to find Cophenetic\n",
    "Correlation Coefficient for particular dataset after running\n",
    "the algorithm. The steps are discussed below\n",
    "1. Run Hierarchical clustering algorithm and find out\n",
    "dendrogram and Cophenetic Correlation\n",
    "Coefficient.\n",
    "2. From dendrogram we can visualize which cluster is\n",
    "outer from other cluster.\n",
    "3. Find those data which belong to this cluster and\n",
    "consider those as outlier.\n",
    "4. Remove those data from dataset and again run\n",
    "Hierarchical algorithm.\n",
    "5. Find Cophenetic Correlation Coefficient and it will\n",
    "be increased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
